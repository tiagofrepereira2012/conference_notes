# AMLD 2025

Tiny ML < 1mW power constraint.

## Tiny ML Audio

Most marketing talk, but they showed the outcome of the paper: https://arxiv.org/pdf/2204.01345
in audio processing.

Key take aways:
    - Peak normalization is an efficient way to normalize audio data for the edge devices

## Tiny ML Vision applications

Hardware platforms:
    - NVIDIA Jetson Nano
    - NVIDIA Orin Nano
    - Coral Dev
    - STM32 Nucleo
    - Raspberry Pi 5
    - GAP9 -- never heard about this one (power consumption 0.01W ops: 0
    - MCU
    
They showed a survey on this pareto curve between power consumption and flops.

Paper presented on the topic: [Paper on Tiny ML Vision applications](https://arxiv.org/pdf/2307.07813).

They presented an interesting eyetracking application on the edge device using event-based cameras and spiing neurons. Custom hardware was used to achieve this. [Retina : Low-Power Eye Tracking with Event Camera and Spiking Hardware](https://arxiv.org/pdf/2312.00425).

The rest of the talk was a bunch of papers their group has published on autonomous driving related stuff, object detection, traffic control, and health monitoring.


## Crash-course on optimizations for TinyML


Developing ML Solution on contex-M MCUs.
They showed a simple pipeline that goals from model training, adaptation and deployment on the edge device.

Shriking techniques:
    - Reducing the number of bits used to represent the weights (quantization)
    - Distilation
        - Incorporating quantization at training time
        - 
    Pruning:
        - Removing weights that are not important
        - Removing layers that are not important

### Quantization
    Two types:
        - Post-training quantization
        - Quantization aware training

    Precision: FP31 to INT8

    Methods
        - Uniform (fixed step) vs Non-uniform (dynamic step)
        - Per tensor (global) vs per channel (layer-wise, better accuracy)

### Distilation
    Teacher-student model was presented

### Pruning
    Removing weights that are not important

    Types:
        - Unsrtuctured- prunes individual weights
        - Structured - prunes entire neurons or layers (easier for the hardware)

    When to prune:
        - After training: 
        - During training


    
## Breaking the memory-compute bottleneck in TinyML

This is the Synthara talk.

 - bla bla bla, Edge AI demands a Beyond Moore's Law solution.
 - He arguees that the memory-compute bottleneck is the main issue in TinyML today.
 - In-Memory Computing promisses 100x better performance and can disrupt the current architectures.
 - They introduce their product: ComputeRAM:
    - Is an SRAM memory
    - Integrates with anay system
    - It contains a computing engine that does matrix multiplications
    - Seamless software integration, with their SDK you can just plug an tf-lite or pytorch model and it will run on the ComputeRAM off-the-shelf.

It was supposed to be a workshop, but it their lawyer didn't allow them to do it, so they just presented
an execution run using a simulation tool. What a pitty.

Well, they showed their CxRPY package where they executed the a matrix multiplication between two numpy arrays
using the CPU and their ComputeRAM and then some reporting comparing the two.

Then they moved to NNs. He presented the MLPerf Tiny benchmark (https://github.com/mlcommons/tiny).
They showed an anomaly detection model using their API that looks like pytorch API.
Their energy consumption is substantially lower than the CPU and other competitors in this MLPerf Tiny benchmark.
API and chips will be released soon.

The take away is if you can cast eveything as a matrix multiplication, you can use their ComputeRAM to speed up your computations.
Cool, stuff. Let's see this in the future.
