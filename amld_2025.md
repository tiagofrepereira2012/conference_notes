# AMLD 2025

Tiny ML < 1mW power constraint.

## Tiny ML Audio

Most marketing talk, but they showed the outcome of the paper: https://arxiv.org/pdf/2204.01345
in audio processing.

Key take aways:
    - Peak normalization is an efficient way to normalize audio data for the edge devices

## Tiny ML Vision applications

Hardware platforms:
    - NVIDIA Jetson Nano
    - NVIDIA Orin Nano
    - Coral Dev
    - STM32 Nucleo
    - Raspberry Pi 5
    - GAP9 -- never heard about this one (power consumption 0.01W ops: 0
    - MCU
    
They showed a survey on this pareto curve between power consumption and flops.

Paper presented on the topic: [Paper on Tiny ML Vision applications](https://arxiv.org/pdf/2307.07813).

They presented an interesting eyetracking application on the edge device using event-based cameras and spiing neurons. Custom hardware was used to achieve this. [Retina : Low-Power Eye Tracking with Event Camera and Spiking Hardware](https://arxiv.org/pdf/2312.00425).

The rest of the talk was a bunch of papers their group has published on autonomous driving related stuff, object detection, traffic control, and health monitoring.


## Crash-course on optimizations for TinyML


Developing ML Solution on contex-M MCUs.
They showed a simple pipeline that goals from model training, adaptation and deployment on the edge device.

Shriking techniques:
    - Reducing the number of bits used to represent the weights (quantization)
    - Distilation
        - Incorporating quantization at training time
        - 
    Pruning:
        - Removing weights that are not important
        - Removing layers that are not important

### Quantization
    Two types:
        - Post-training quantization
        - Quantization aware training

    Precision: FP31 to INT8

    Methods
        - Uniform (fixed step) vs Non-uniform (dynamic step)
        - Per tensor (global) vs per channel (layer-wise, better accuracy)

### Distilation
    Teacher-student model was presented

### Pruning
    Removing weights that are not important

    Types:
        - Unsrtuctured- prunes individual weights
        - Structured - prunes entire neurons or layers (easier for the hardware)

    When to prune:
        - After training: 
        - During training


    



